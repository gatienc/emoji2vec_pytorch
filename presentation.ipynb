{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check out the original [tensorflow implementation](https://github.com/uclmr/emoji2vec) of Emoji2Vec by its authors as well as the [paper](https://arxiv.org/pdf/1609.08359.pdf) for more details. <br>\n",
    "This notebook is intended to provide an intuition behind Emoji2Vec and provide an idea of its features rather than serve as a in-depth analysis of the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import gensim.models as gsm\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "#for TSNE Visualization\n",
    "import pandas as pd \n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Internal dependencies\n",
    "from model import Emoji2Vec, ModelParams\n",
    "from phrase2vec import Phrase2Vec\n",
    "from utils import build_kb, get_examples_from_kb, generate_embeddings, get_metrics, generate_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Emoji2Vec?\n",
    "Emoji2Vec representations are a form of distributed representations for Unicode emojis, trained directly from natural language descriptions of emojis. <br>\n",
    "This notebook presents a high level walkthrough the process of training Emoji2Vec representations and demonstrates some experimental results. <br><br>\n",
    "First, lets define the logical steps in training Emoji2Vec representations.\n",
    "1. For each emoji in a data set a number of natural language descriptions is collected.\n",
    "2. Each description is encoded to a fixed form vector in a high dimensional space. \n",
    "    - Although it can be done by an arbitrary encoding method, this implementation follows the approach presented in the [paper](https://arxiv.org/pdf/1609.08359.pdf) using 300-dimensional [Google News word2vec embeddings](https://code.google.com/archive/p/word2vec/) together with a simple phrase encoder `phrase2vec.Phrase2Vec`.\n",
    "3. A neural network model is trained to classify emojis from their descriptions.\n",
    "    - Inside the model each unique emoji has its own vector of parameters that is updated when this emoji is being classified. Through continuous incrementation in training such vectors of parameters become emoji representations.\n",
    "4. Neural network's parameters are extracted and used as distributed emoji representations that are embedded in the same space as the underlying word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train Emoji2Vec representations?\n",
    "First, we need a couple of things to train Emoji2Vec representations. Most importantly we need something to encode natural language descriptions of emojis to a fixed form high dimensional vectors. For this we are going to use [Google News wor2vec embeddings](https://code.google.com/archive/p/word2vec/), so in order to continue with this notebook make sure you've downloaded those embeddings and placed a `.bin.gz` file in `data/word2vec`. <br><br>\n",
    "Of course we also need some emojis and their natural language descriptions. Fortunately authors of the paper collected a data set that is part of this repository. <br>\n",
    "Our training data set is in `data/training`. Let's add this directory to our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"data_folder\": \"data/training\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train Emoji2Vec representations we need to put emojis in some sort of organised structure, so that after training we know which vector of parameters in the model is associated with which emoji. Of course the same needs to be done for each phrase (natural language description of emoji) in the data set so that we know which descrption describes which emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training data from: data/training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('reading training data from: ' + params[\"data_folder\"])\n",
    "kb, ind2phr, ind2emoji = build_kb(params[\"data_folder\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need this emoji mapping later, so for now let's save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"mapping_file\": \"emoji_mapping_file.pkl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mapping from index to emoji\n",
    "pk.dump(ind2emoji, open(params[\"mapping_file\"], 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to encode descriptions of emojis to fixed form vectors. As mentioned before we're using 300 dimensional Google News embeddings located in `data/word2vec`. As we will need them later we're going to save the generated embeddings of descriptions to a file `phrase_embeddings.pkl`. <br>Let's add those paths to our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"word2vec_embeddings_file\": \"data/word2vec/GoogleNews-vectors-negative300.bin.gz\",\n",
    "               \"phrase_embeddings_file\": \"phrase_embeddings.pkl\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reitarete - phrase embeddings are embeddigns of each emoji description that will serve as input for the neural network, whereas Google News embeddings are what we use to generate phrase embeddings. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate embeddings for each phrase in the training set\n",
    "embeddings_array = generate_embeddings(ind2phr=ind2phr, kb=kb, embeddings_file=params[\"phrase_embeddings_file\"],\n",
    "                                    word2vec_file=params[\"word2vec_embeddings_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the training input (phrase embeddings), now let's load our training and development data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_examples_from_kb(kb=kb, example_type='train')\n",
    "dev_set = get_examples_from_kb(kb=kb, example_type='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_params = ModelParams(in_dim=300, \n",
    "                        out_dim=300, \n",
    "                        max_epochs=60, \n",
    "                        pos_ex=4, \n",
    "                        neg_ratio=1, \n",
    "                        learning_rate=0.001,\n",
    "                        dropout=0.0, \n",
    "                        class_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, it needs to know the training parameters, the number of emojis (size of parameter matrix depends on it) and the matrix of phrase embeddings (embeddings of emoji descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Emoji2Vec(model_params=model_params, num_emojis=kb.dim_size(0), embeddings_array=embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been defined, let's train it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train :\n",
    "    model.train(kb=kb, epochs=model_params.max_epochs, learning_rate=model_params.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the trained PyTorch model to a file so that we can load it later if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train :\n",
    "    model_folder = 'example_emoji2vec'\n",
    "    if not os.path.isdir(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    \n",
    "    torch.save(model.nn, model_folder + '/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train==False :\n",
    "    model_folder = 'example_emoji2vec'\n",
    "    model.nn = torch.load(model_folder + '/model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and saved we need to extract emoji representations from the neural network's parameters. <br>\n",
    "This can be done by a `model.Emoji2Vec` method called `create_gensim_files`, which will save the distributed representations of emojis in a format compatible with `gensim.models` allowing us to add Emoji2Vec representations to a gensim model and use them as any other word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2v = model.create_gensim_files(model_folder=model_folder, ind2emoj=ind2emoji, out_dim=model_params.out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done.\n",
    "That's it, you've generated your own Emoji2Vec representations which now sit in `example_emoji2vec/` as `emoji2vec.txt` and `emoji2vec.bin`. Good job. <br>\n",
    "Now is the time for the fun part, for example we can create a gensim model with our newly generated emoji embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2v = gsm.KeyedVectors.load_word2vec_format(\"example_emoji2vec/emoji2vec.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the emojis in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [key for key in e2v.key_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ„ğŸ»' 'ğŸ‚' 'ğŸš™' 'ğŸ“š' 'ğŸ•³' 'ğŸ„' 'ğŸ‡¸ğŸ‡»' 'ğŸ¿' 'ğŸ‡ªğŸ‡·' 'ğŸ˜’']\n"
     ]
    }
   ],
   "source": [
    "# Sample 10 random emojis from the data set.\n",
    "example_emojis = np.random.choice(list(vocabulary), 10)\n",
    "print(example_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above emojis has its own vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08201232, -0.36415255, -0.24427035, ...,  0.10918477,\n",
       "        -0.03226461, -0.21312062],\n",
       "       [ 0.01127965,  0.5134385 ,  0.05798573, ...,  0.74110353,\n",
       "         0.82430106,  0.05123174],\n",
       "       [ 0.4363332 , -0.341146  ,  0.10862169, ...,  0.288362  ,\n",
       "        -0.19319369, -0.80230993],\n",
       "       ...,\n",
       "       [-0.3499136 , -0.00729424, -0.40118662, ...,  0.34328744,\n",
       "         0.18415043,  0.07090758],\n",
       "       [ 0.0113311 , -0.15118903,  0.14767584, ..., -0.0274291 ,\n",
       "         0.10146958, -0.11366049],\n",
       "       [ 0.41253617,  0.25854504, -0.2080431 , ...,  0.47998717,\n",
       "        -0.14779189, -0.24613853]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v[example_emojis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the fact that the learnt emoji representations are compatible with `gensim.models`, we can do a lot of cool things like finding similar emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸš“', 0.8738017082214355),\n",
       " ('ğŸš‘', 0.6391749382019043),\n",
       " ('ğŸš—', 0.5790260434150696),\n",
       " ('ğŸ‘®ğŸ¿', 0.5675467252731323),\n",
       " ('ğŸš˜', 0.5655881762504578),\n",
       " ('ğŸš', 0.5369750261306763),\n",
       " ('ğŸš™', 0.5357747673988342),\n",
       " ('ğŸ‘®', 0.5355160236358643),\n",
       " ('ğŸšƒ', 0.525036633014679),\n",
       " ('ğŸš’', 0.5179762244224548)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸš”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ¦‹', 0.5947760939598083),\n",
       " ('ğŸ›€', 0.5851469039916992),\n",
       " ('ğŸŒŠ', 0.5719010233879089),\n",
       " ('ğŸŠğŸ»', 0.5664109587669373),\n",
       " ('ğŸ¤¾', 0.5615712404251099),\n",
       " ('ğŸƒ', 0.5562608242034912),\n",
       " ('ğŸš°', 0.5495843887329102),\n",
       " ('ğŸŠğŸ¼', 0.5461294054985046),\n",
       " ('ğŸš¿', 0.5455986857414246),\n",
       " ('ğŸ’¦', 0.5421949028968811)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸŠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸš…', 0.8224267363548279),\n",
       " ('ğŸš‹', 0.680420994758606),\n",
       " ('ğŸš†', 0.678656280040741),\n",
       " ('ğŸšƒ', 0.651321530342102),\n",
       " ('ğŸš‰', 0.6461228728294373),\n",
       " ('ğŸšŸ', 0.6257645487785339),\n",
       " ('ğŸšˆ', 0.6000259518623352),\n",
       " ('ğŸš‚', 0.5937529802322388),\n",
       " ('ğŸš ', 0.5721362829208374),\n",
       " ('ğŸš', 0.5674235224723816)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸš„')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â˜ï¸', 0.6689046025276184),\n",
       " ('â›ˆ', 0.6365997791290283),\n",
       " ('â›…ï¸', 0.6132184267044067),\n",
       " ('ğŸŒ¨', 0.6071565747261047),\n",
       " ('ğŸŒª', 0.581260085105896),\n",
       " ('ğŸŒ§', 0.5776381492614746),\n",
       " ('ğŸŒ¤', 0.5693185329437256),\n",
       " ('ğŸŒ¥', 0.5634979605674744),\n",
       " ('â›…', 0.5478359460830688),\n",
       " ('ğŸŒ¦', 0.5307491421699524)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸŒ©')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Emoji2Vec' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vocab\u001b[39m=\u001b[39mvocabulary\n\u001b[1;32m      2\u001b[0m \u001b[39m# embedding from first model layer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39;49mparameters())[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m \u001b[39m# normalization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m norms \u001b[39m=\u001b[39m (embeddings \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Emoji2Vec' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "vocab=vocabulary\n",
    "# embedding from first model layer\n",
    "embeddings = list(model.parameters())[0].detach().numpy()\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "#pas compris\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape\n",
    "# get embeddings\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# t-SNE transform\n",
    "tsne = TSNE(n_components=2)\n",
    "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
    "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
    "\n",
    "# get token order\n",
    "embeddings_df_trans.index = vocab.get_itos()\n",
    "\n",
    "# if token is a number\n",
    "is_numeric = embeddings_df_trans.index.str.isnumeric()\n",
    "\n",
    "color = np.where(is_numeric, \"green\", \"black\")\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=embeddings_df_trans[0],\n",
    "        y=embeddings_df_trans[1],\n",
    "        mode=\"text\",\n",
    "        text=embeddings_df_trans.index,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(color=color),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"../word2vec_visualization.html\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# get embeddings\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# t-SNE transform\n",
    "tsne = TSNE(n_components=2)\n",
    "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
    "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
    "\n",
    "# get token order\n",
    "embeddings_df_trans.index = vocab.get_itos()\n",
    "\n",
    "# if token is a number\n",
    "is_numeric = embeddings_df_trans.index.str.isnumeric()\n",
    "\n",
    "color = np.where(is_numeric, \"green\", \"black\")\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=embeddings_df_trans[0],\n",
    "        y=embeddings_df_trans[1],\n",
    "        mode=\"text\",\n",
    "        text=embeddings_df_trans.index,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(color=color),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"../word2vec_visualization.html\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the real fun starts when we combine the power of emoji embeddings with word embeddings. <br>\n",
    "Let's create a model that combines the two, this will allow us to measure similarity between emojis and phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/numpy/core/numerictypes.py:319\u001b[0m, in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39missubclass\u001b[39;49m(arg1, arg2)\n\u001b[1;32m    320\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m phraseVecModel \u001b[39m=\u001b[39m Phrase2Vec\u001b[39m.\u001b[39;49mfrom_word2vec_paths(\u001b[39m300\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mdata/word2vec/GoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                                                 \u001b[39m\"\u001b[39;49m\u001b[39mexample_model/emoji2vec.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/emoji2vec_pytorch/phrase2vec.py:60\u001b[0m, in \u001b[0;36mPhrase2Vec.from_word2vec_paths\u001b[0;34m(cls, dim, w2v_path, e2v_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(w2v_path):\n\u001b[1;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     53\u001b[0m         \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     54\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not found. Either provide a different path, or download binary from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m     )\n\u001b[0;32m---> 60\u001b[0m w2v \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(w2v_path, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m e2v_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     e2v \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39mKeyedVectors\u001b[39m.\u001b[39mload_word2vec_format(e2v_path, binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[39m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1960\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1958\u001b[0m new_chunk \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39mread(binary_chunk_size)\n\u001b[1;32m   1959\u001b[0m chunk \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_chunk\n\u001b[0;32m-> 1960\u001b[0m processed_words, chunk \u001b[39m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m     kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   1962\u001b[0m tot_processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m processed_words\n\u001b[1;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_chunk) \u001b[39m<\u001b[39m binary_chunk_size:\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1943\u001b[0m, in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1941\u001b[0m word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1942\u001b[0m vector \u001b[39m=\u001b[39m frombuffer(chunk, offset\u001b[39m=\u001b[39mi_vector, count\u001b[39m=\u001b[39mvector_size, dtype\u001b[39m=\u001b[39mREAL)\u001b[39m.\u001b[39mastype(datatype)\n\u001b[0;32m-> 1943\u001b[0m _add_word_to_kv(kv, counts, word, vector, vocab_size)\n\u001b[1;32m   1944\u001b[0m start \u001b[39m=\u001b[39m i_vector \u001b[39m+\u001b[39m bytes_per_vector\n\u001b[1;32m   1945\u001b[0m processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1923\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mvocabulary file is incomplete: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is missing\u001b[39m\u001b[39m\"\u001b[39m, word)\n\u001b[1;32m   1922\u001b[0m     word_count \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1923\u001b[0m kv\u001b[39m.\u001b[39;49mset_vecattr(word, \u001b[39m'\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m'\u001b[39;49m, word_count)\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:353\u001b[0m, in \u001b[0;36mKeyedVectors.set_vecattr\u001b[0;34m(self, key, attr, val)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_vecattr\u001b[39m(\u001b[39mself\u001b[39m, key, attr, val):\n\u001b[1;32m    335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set attribute associated with the given key to value.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallocate_vecattrs(attrs\u001b[39m=\u001b[39;49m[attr], types\u001b[39m=\u001b[39;49m[\u001b[39mtype\u001b[39;49m(val)])\n\u001b[1;32m    354\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key)\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpandos[attr][index] \u001b[39m=\u001b[39m val\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:323\u001b[0m, in \u001b[0;36mKeyedVectors.allocate_vecattrs\u001b[0;34m(self, attrs, types)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m prev_expando \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpandos[attr]\n\u001b[0;32m--> 323\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39;49missubdtype(t, prev_expando\u001b[39m.\u001b[39;49mdtype):\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt allocate type \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m for attribute \u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconflicts with its existing type \u001b[39m\u001b[39m{\u001b[39;00mprev_expando\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prev_expando) \u001b[39m==\u001b[39m target_size:\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/numpy/core/numerictypes.py:417\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[1;32m    416\u001b[0m     arg1 \u001b[39m=\u001b[39m dtype(arg1)\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    418\u001b[0m     arg2 \u001b[39m=\u001b[39m dtype(arg2)\u001b[39m.\u001b[39mtype\n\u001b[1;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39missubclass\u001b[39m(arg1, arg2)\n",
      "File \u001b[0;32m~/TPS/Language-model-discovery/NLPenv/lib/python3.10/site-packages/numpy/core/numerictypes.py:319\u001b[0m, in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39mDetermine if a class is a subclass of a second class.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \n\u001b[1;32m    317\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39missubclass\u001b[39;49m(arg1, arg2)\n\u001b[1;32m    320\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phraseVecModel = Phrase2Vec.from_word2vec_paths(300,\n",
    "                                                \"data/word2vec/GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                                \"example_model/emoji2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from id to emoji\n",
    "mapping = pk.load(open(params[\"mapping_file\"], 'rb'))\n",
    "# mapping from emoji to id\n",
    "inverse_mapping = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogous_emoji(emoji_1,\n",
    "                         emoji_2,\n",
    "                         emoji_3,\n",
    "                         top_n,\n",
    "                         mapping=mapping,\n",
    "                         inverse_mapping=inverse_mapping,\n",
    "                         e2v=e2v,\n",
    "                         model=model):\n",
    "    similarities = []\n",
    "    vector = e2v[emoji_1] - e2v[emoji_2] + e2v[emoji_3]\n",
    "    vector = vector / np.linalg.norm(vector)\n",
    "    \n",
    "    for idx in range(len(mapping)):\n",
    "        emoij_idx_similarity = model.nn.forward(torch.Tensor(vector.reshape(1, -1)), idx).detach().numpy()\n",
    "        similarities.append(emoij_idx_similarity)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    n_most_similar_idxs = similarities.argsort(axis=0)[-top_n:][::-1].reshape(-1)\n",
    "    n_most_similar_emojis = [mapping[emoji_idx] for emoji_idx in n_most_similar_idxs]\n",
    "    \n",
    "    str_expression = ' '.join([emoji_1, \"-\", emoji_2, \"+\", emoji_3,])\n",
    "    top_score = similarities[n_most_similar_idxs[0]]\n",
    "    \n",
    "    return str_expression, n_most_similar_emojis\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"Expression\", \"Closest emojis\"],\n",
    "         [*find_analogous_emoji(\"ğŸ¤´\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ‘‘\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ‘¦\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’µ\", \"ğŸ‡ºğŸ‡¸\", \"ğŸ‡¬ğŸ‡§\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’µ\", \"ğŸ‡ºğŸ‡¸\", \"ğŸ‡ªğŸ‡º\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’·\", \"ğŸ‡¬ğŸ‡§\", \"ğŸ‡ªğŸ‡º\", 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============  ==================\n",
      "Expression    Closest emojis\n",
      "============  ==================\n",
      "ğŸ¤´ - ğŸš¹ + ğŸšº  ['ğŸ‘¸', 'ğŸšº', 'ğŸ°']\n",
      "ğŸ‘‘ - ğŸš¹ + ğŸšº  ['ğŸ‘‘', 'ğŸ‘¸', 'ğŸ°']\n",
      "ğŸ‘¦ - ğŸš¹ + ğŸšº  ['ğŸšº', 'ğŸ‘¸', 'ğŸ£']\n",
      "ğŸ’µ - ğŸ‡ºğŸ‡¸ + ğŸ‡¬ğŸ‡§  ['ğŸ’µ', 'ğŸ’·', 'ğŸ’¶']\n",
      "ğŸ’µ - ğŸ‡ºğŸ‡¸ + ğŸ‡ªğŸ‡º  ['ğŸ’µ', 'ğŸ’¶', 'ğŸ’´']\n",
      "ğŸ’· - ğŸ‡¬ğŸ‡§ + ğŸ‡ªğŸ‡º  ['ğŸ’·', 'ğŸ’¶', 'ğŸ’µ']\n",
      "============  ==================\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"rst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although far from perfect, similarly to the [published](https://arxiv.org/pdf/1609.08359.pdf) results an emoji considered to be \"right\" is usually within the top 3 examples selected by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Emoji2Vec representations with word2vec embeddings they were trained on also allows to inspect the relationships between word embeddings and emoji embeddings. For example with the `phrase2emoji` function below we can see emojis most similar to an arbitrary phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase2emoji(phrase, \n",
    "                 top_n,\n",
    "                 phraseVecModel=phraseVecModel,\n",
    "                 mapping=mapping,\n",
    "                 model=model):\n",
    "    \n",
    "    similarities = []\n",
    "    phrase_vec = phraseVecModel[phrase]\n",
    "    for idx in range(len(mapping)):\n",
    "        emoij_idx_similarity = model.nn.forward(torch.Tensor(phrase_vec.reshape(1, -1)), idx).detach().numpy()\n",
    "        similarities.append(emoij_idx_similarity)\n",
    "        \n",
    "    similarities = np.array(similarities)\n",
    "    n_most_similar_idxs = similarities.argsort(axis=0)[-top_n:][::-1].reshape(-1)\n",
    "    n_most_similar_emojis = [mapping[emoji_idx] for emoji_idx in n_most_similar_idxs]\n",
    "    \n",
    "    return phrase, n_most_similar_emojis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"Phrase\", \"Closest emojis\"],\n",
    "         [*phrase2emoji(\"funny\", 3)],\n",
    "         [*phrase2emoji(\"scary\", 3)],\n",
    "         [*phrase2emoji(\"okay\", 3)],\n",
    "         [*phrase2emoji(\"crazy\", 3)],\n",
    "         [*phrase2emoji(\"wild\", 3)],\n",
    "         [*phrase2emoji(\"afraid\", 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========  ==================\n",
      "Phrase    Closest emojis\n",
      "========  ==================\n",
      "funny     ['ğŸ˜œ', 'ğŸ‘…', 'ğŸ˜›']\n",
      "scary     ['ğŸ‘¹', 'ğŸƒ', 'ğŸ˜œ']\n",
      "okay      ['ğŸ‘Œ', 'ğŸ†—', 'ğŸ™†']\n",
      "crazy     ['ğŸ˜œ', 'ğŸ˜', 'ğŸ’­']\n",
      "wild      ['ğŸŒµ', 'ğŸ—', 'ğŸ†']\n",
      "afraid    ['ğŸ˜¨', 'ğŸ˜Ÿ', 'ğŸ˜']\n",
      "========  ==================\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"rst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to exist a strong relationship between simple adjectives and emojis. That's great, but is this the whole point of Emoji2Vec? <br>\n",
    "Of course not. In fact Emoji2Vec authors evaluated their method's performance on a downstream task of sentiment classification on a dataset  by  Kralj   Novak  et  al.  (2015),  which  consists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment. Using Emoji2Vec alongside word embeddings, as opposed to using just word embeddings, yielded an improvement in classification accuracy across all studied datasets. In the same task Emoji2Vec also outperformed an alternative method for emoji representation. <br>\n",
    "For more details check out the [paper](https://arxiv.org/pdf/1609.08359.pdf). You can also inspect the results of the sentiment classification task in [this notebook](https://github.com/uclmr/emoji2vec/blob/master/TwitterClassification.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
