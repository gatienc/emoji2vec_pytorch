{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check out the original [tensorflow implementation](https://github.com/uclmr/emoji2vec) of Emoji2Vec by its authors as well as the [paper](https://arxiv.org/pdf/1609.08359.pdf) for more details. <br>\n",
    "This notebook is intended to provide an intuition behind Emoji2Vec and provide an idea of its features rather than serve as a in-depth analysis of the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import gensim.models as gsm\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "#for TSNE Visualization\n",
    "import pandas as pd \n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Internal dependencies\n",
    "from model import Emoji2Vec, ModelParams\n",
    "from phrase2vec import Phrase2Vec\n",
    "from utils import build_kb, get_examples_from_kb, generate_embeddings, get_metrics, generate_predictions\n",
    "from visualize import visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Emoji2Vec?\n",
    "Emoji2Vec representations are a form of distributed representations for Unicode emojis, trained directly from natural language descriptions of emojis. <br>\n",
    "This notebook presents a high level walkthrough the process of training Emoji2Vec representations and demonstrates some experimental results. <br><br>\n",
    "First, lets define the logical steps in training Emoji2Vec representations.\n",
    "1. For each emoji in a data set a number of natural language descriptions is collected.\n",
    "2. Each description is encoded to a fixed form vector in a high dimensional space. \n",
    "    - Although it can be done by an arbitrary encoding method, this implementation follows the approach presented in the [paper](https://arxiv.org/pdf/1609.08359.pdf) using 300-dimensional [Google News word2vec embeddings](https://code.google.com/archive/p/word2vec/) together with a simple phrase encoder `phrase2vec.Phrase2Vec`.\n",
    "3. A neural network model is trained to classify emojis from their descriptions.\n",
    "    - Inside the model each unique emoji has its own vector of parameters that is updated when this emoji is being classified. Through continuous incrementation in training such vectors of parameters become emoji representations.\n",
    "4. Neural network's parameters are extracted and used as distributed emoji representations that are embedded in the same space as the underlying word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train Emoji2Vec representations?\n",
    "First, we need a couple of things to train Emoji2Vec representations. Most importantly we need something to encode natural language descriptions of emojis to a fixed form high dimensional vectors. For this we are going to use [Google News wor2vec embeddings](https://code.google.com/archive/p/word2vec/), so in order to continue with this notebook make sure you've downloaded those embeddings and placed a `.bin.gz` file in `data/word2vec`. <br><br>\n",
    "Of course we also need some emojis and their natural language descriptions. Fortunately authors of the paper collected a data set that is part of this repository. <br>\n",
    "Our training data set is in `data/training`. Let's add this directory to our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"data_folder\": \"data/training\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train Emoji2Vec representations we need to put emojis in some sort of organised structure, so that after training we know which vector of parameters in the model is associated with which emoji. Of course the same needs to be done for each phrase (natural language description of emoji) in the data set so that we know which descrption describes which emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training data from: data/training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('reading training data from: ' + params[\"data_folder\"])\n",
    "kb, ind2phr, ind2emoji = build_kb(params[\"data_folder\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need this emoji mapping later, so for now let's save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"mapping_file\": \"emoji_mapping_file.pkl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mapping from index to emoji\n",
    "pk.dump(ind2emoji, open(params[\"mapping_file\"], 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to encode descriptions of emojis to fixed form vectors. As mentioned before we're using 300 dimensional Google News embeddings located in `data/word2vec`. As we will need them later we're going to save the generated embeddings of descriptions to a file `phrase_embeddings.pkl`. <br>Let's add those paths to our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\"word2vec_embeddings_file\": \"data/word2vec/GoogleNews-vectors-negative300.bin.gz\",\n",
    "               \"phrase_embeddings_file\": \"phrase_embeddings.pkl\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reitarete - phrase embeddings are embeddigns of each emoji description that will serve as input for the neural network, whereas Google News embeddings are what we use to generate phrase embeddings. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate embeddings for each phrase in the training set\n",
    "embeddings_array = generate_embeddings(ind2phr=ind2phr, kb=kb, embeddings_file=params[\"phrase_embeddings_file\"],\n",
    "                                    word2vec_file=params[\"word2vec_embeddings_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the training input (phrase embeddings), now let's load our training and development data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_examples_from_kb(kb=kb, example_type='train')\n",
    "dev_set = get_examples_from_kb(kb=kb, example_type='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_params = ModelParams(in_dim=300, \n",
    "                        out_dim=300, \n",
    "                        max_epochs=60, \n",
    "                        pos_ex=4, \n",
    "                        neg_ratio=1, \n",
    "                        learning_rate=0.001,\n",
    "                        dropout=0.0, \n",
    "                        class_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, it needs to know the training parameters, the number of emojis (size of parameter matrix depends on it) and the matrix of phrase embeddings (embeddings of emoji descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Emoji2Vec(model_params=model_params, num_emojis=kb.dim_size(0), embeddings_array=embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been defined, let's train it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train :\n",
    "    model.train(kb=kb, epochs=model_params.max_epochs, learning_rate=model_params.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the trained PyTorch model to a file so that we can load it later if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train :\n",
    "    model_folder = 'example_emoji2vec'\n",
    "    if not os.path.isdir(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "    \n",
    "    torch.save(model.nn, model_folder + '/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train==False :\n",
    "    model_folder = 'example_emoji2vec'\n",
    "    model.nn = torch.load(model_folder + '/model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained and saved we need to extract emoji representations from the neural network's parameters. <br>\n",
    "This can be done by a `model.Emoji2Vec` method called `create_gensim_files`, which will save the distributed representations of emojis in a format compatible with `gensim.models` allowing us to add Emoji2Vec representations to a gensim model and use them as any other word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2v = model.create_gensim_files(model_folder=model_folder, ind2emoj=ind2emoji, out_dim=model_params.out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done.\n",
    "That's it, you've generated your own Emoji2Vec representations which now sit in `example_emoji2vec/` as `emoji2vec.txt` and `emoji2vec.bin`. Good job. <br>\n",
    "Now is the time for the fun part, for example we can create a gensim model with our newly generated emoji embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2v = gsm.KeyedVectors.load_word2vec_format(\"example_emoji2vec/emoji2vec.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the emojis in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [key for key in e2v.key_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ‘œ' 'ğŸ“•' 'ğŸ‡¬ğŸ‡·' 'â›…ï¸' 'ğŸ“«' 'â™' 'ğŸ¤' 'ğŸ‰' 'ğŸ‡¬ğŸ‡¾' 'ğŸ”£']\n"
     ]
    }
   ],
   "source": [
    "# Sample 10 random emojis from the data set.\n",
    "example_emojis = np.random.choice(list(vocabulary), 10)\n",
    "print(example_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above emojis has its own vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3679468 ,  0.00168402, -0.4711585 , ...,  0.11752382,\n",
       "        -0.4276958 , -0.12119073],\n",
       "       [-0.15438907, -0.4375898 , -0.09648127, ...,  0.6098831 ,\n",
       "        -0.01111347,  0.03629427],\n",
       "       [-0.00474668,  0.13680199,  0.19537875, ..., -0.08347146,\n",
       "        -0.1081543 , -0.02951165],\n",
       "       ...,\n",
       "       [-0.85650986, -0.5421017 ,  0.12751018, ...,  0.03175882,\n",
       "        -0.6157671 , -0.05484686],\n",
       "       [-0.21084985, -0.24620818,  0.0832018 , ..., -0.01567112,\n",
       "         0.09533603, -0.05419327],\n",
       "       [ 0.22814782, -0.29438126, -0.23147367, ..., -0.41529545,\n",
       "         0.14431834, -0.06887833]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v[example_emojis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the fact that the learnt emoji representations are compatible with `gensim.models`, we can do a lot of cool things like finding similar emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸš“', 0.8738017082214355),\n",
       " ('ğŸš‘', 0.6391749382019043),\n",
       " ('ğŸš—', 0.5790260434150696),\n",
       " ('ğŸ‘®ğŸ¿', 0.5675467252731323),\n",
       " ('ğŸš˜', 0.5655882358551025),\n",
       " ('ğŸš', 0.5369750261306763),\n",
       " ('ğŸš™', 0.5357747673988342),\n",
       " ('ğŸ‘®', 0.5355160236358643),\n",
       " ('ğŸšƒ', 0.525036633014679),\n",
       " ('ğŸš’', 0.5179762244224548)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸš”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ¦‹', 0.5947760939598083),\n",
       " ('ğŸ›€', 0.5851469039916992),\n",
       " ('ğŸŒŠ', 0.5719010233879089),\n",
       " ('ğŸŠğŸ»', 0.5664109587669373),\n",
       " ('ğŸ¤¾', 0.5615712404251099),\n",
       " ('ğŸƒ', 0.5562608242034912),\n",
       " ('ğŸš°', 0.5495843887329102),\n",
       " ('ğŸŠğŸ¼', 0.5461294054985046),\n",
       " ('ğŸš¿', 0.5455986857414246),\n",
       " ('ğŸ’¦', 0.5421949028968811)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸŠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸš…', 0.8224267363548279),\n",
       " ('ğŸš‹', 0.680420994758606),\n",
       " ('ğŸš†', 0.678656280040741),\n",
       " ('ğŸšƒ', 0.651321530342102),\n",
       " ('ğŸš‰', 0.6461228728294373),\n",
       " ('ğŸšŸ', 0.6257645487785339),\n",
       " ('ğŸšˆ', 0.6000259518623352),\n",
       " ('ğŸš‚', 0.5937529802322388),\n",
       " ('ğŸš ', 0.5721362829208374),\n",
       " ('ğŸš', 0.5674235224723816)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸš„')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ˜¹', 0.726284921169281),\n",
       " ('ğŸ˜†', 0.6457535028457642),\n",
       " ('ğŸ˜¢', 0.6286451816558838),\n",
       " ('ğŸ˜Š', 0.6132701635360718),\n",
       " ('ğŸ˜­', 0.5730255246162415),\n",
       " ('ğŸ˜€', 0.5688401460647583),\n",
       " ('â˜ºï¸', 0.561689019203186),\n",
       " ('ğŸ˜¨', 0.5600235462188721),\n",
       " ('ğŸ˜ƒ', 0.55708909034729),\n",
       " ('ğŸ™‚', 0.5358986258506775)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar('ğŸ˜‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤´ - ğŸ‘¨ + ğŸ‘© (= ğŸ‘¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘‘', 0.5246703028678894),\n",
       " ('ğŸ‘¸', 0.5064293742179871),\n",
       " ('ğŸ‘¸ğŸ¼', 0.456595778465271),\n",
       " ('ğŸ’‚', 0.42242008447647095),\n",
       " ('ğŸ’', 0.40975672006607056),\n",
       " ('ğŸ‡®ğŸ‡¨', 0.4070007801055908),\n",
       " ('ğŸ’‹', 0.40582364797592163),\n",
       " ('ğŸ¤°', 0.40565934777259827),\n",
       " ('ğŸ“¿', 0.405147910118103),\n",
       " ('ğŸ‘¯', 0.40252700448036194)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v.most_similar(positive=['ğŸ‘©', 'ğŸ¤´'], negative=['ğŸ‘¨'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-d DIR] [-w WORD] [-m MAPPING]\n",
      "                             [-em EMBEDDINGS] [-k DIM] [-b BATCH] [-e EPOCHS]\n",
      "                             [-r RATIO] [-l LEARNING] [-dr DROPOUT]\n",
      "                             [-t THRESHOLD] [-ds DATASET] [-D DEBUG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9035 --control=9033 --hb=9032 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"927b071a-a9e0-47be-8e21-6dd60b86a55e\" --shell=9034 --transport=\"tcp\" --iopub=9036 --f=/home/gchenu/.local/share/jupyter/runtime/kernel-v2-18823dtYtlKU0cwq.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "visualize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the real fun starts when we combine the power of emoji embeddings with word embeddings. <br>\n",
    "Let's create a model that combines the two, this will allow us to measure similarity between emojis and phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraseVecModel = Phrase2Vec.from_word2vec_paths(300,\n",
    "                                                \"data/word2vec/GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                                \"example_emoji2vec/emoji2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from id to emoji\n",
    "mapping = pk.load(open(params[\"mapping_file\"], 'rb'))\n",
    "# mapping from emoji to id\n",
    "inverse_mapping = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogous_emoji(emoji_1,\n",
    "                         emoji_2,\n",
    "                         emoji_3,\n",
    "                         top_n,\n",
    "                         mapping=mapping,\n",
    "                         inverse_mapping=inverse_mapping,\n",
    "                         e2v=e2v,\n",
    "                         model=model):\n",
    "    similarities = []\n",
    "    vector = e2v[emoji_1] - e2v[emoji_2] + e2v[emoji_3]\n",
    "    vector = vector / np.linalg.norm(vector)\n",
    "    \n",
    "    for idx in range(len(mapping)):\n",
    "        emoij_idx_similarity = model.nn.forward(torch.Tensor(vector.reshape(1, -1)), idx).detach().numpy()\n",
    "        similarities.append(emoij_idx_similarity)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    n_most_similar_idxs = similarities.argsort(axis=0)[-top_n:][::-1].reshape(-1)\n",
    "    n_most_similar_emojis = [mapping[emoji_idx] for emoji_idx in n_most_similar_idxs]\n",
    "    \n",
    "    str_expression = ' '.join([emoji_1, \"-\", emoji_2, \"+\", emoji_3,])\n",
    "    top_score = similarities[n_most_similar_idxs[0]]\n",
    "    \n",
    "    return str_expression, n_most_similar_emojis\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"Expression\", \"Closest emojis\"],\n",
    "         [*find_analogous_emoji(\"ğŸ¤´\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ‘‘\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ‘¦\", \"ğŸš¹\", \"ğŸšº\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’µ\", \"ğŸ‡ºğŸ‡¸\", \"ğŸ‡¬ğŸ‡§\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’µ\", \"ğŸ‡ºğŸ‡¸\", \"ğŸ‡ªğŸ‡º\", 3)],\n",
    "         [*find_analogous_emoji(\"ğŸ’·\", \"ğŸ‡¬ğŸ‡§\", \"ğŸ‡ªğŸ‡º\", 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============  ==================\n",
      "Expression    Closest emojis\n",
      "============  ==================\n",
      "ğŸ¤´ - ğŸš¹ + ğŸšº  ['ğŸšº', 'ğŸ‘¸', 'ğŸ‘ª']\n",
      "ğŸ‘‘ - ğŸš¹ + ğŸšº  ['ğŸ‘‘', 'ğŸ‘¸', 'ğŸ°']\n",
      "ğŸ‘¦ - ğŸš¹ + ğŸšº  ['ğŸ‘ª', 'ğŸšº', 'ğŸ‘¸']\n",
      "ğŸ’µ - ğŸ‡ºğŸ‡¸ + ğŸ‡¬ğŸ‡§  ['ğŸ’µ', 'ğŸ’·', 'ğŸ’´']\n",
      "ğŸ’µ - ğŸ‡ºğŸ‡¸ + ğŸ‡ªğŸ‡º  ['ğŸ’µ', 'ğŸ’·', 'ğŸ’¶']\n",
      "ğŸ’· - ğŸ‡¬ğŸ‡§ + ğŸ‡ªğŸ‡º  ['ğŸ’·', 'ğŸ’¶', 'ğŸ’µ']\n",
      "============  ==================\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"rst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although far from perfect, similarly to the [published](https://arxiv.org/pdf/1609.08359.pdf) results an emoji considered to be \"right\" is usually within the top 3 examples selected by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Emoji2Vec representations with word2vec embeddings they were trained on also allows to inspect the relationships between word embeddings and emoji embeddings. For example with the `phrase2emoji` function below we can see emojis most similar to an arbitrary phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase2emoji(phrase, \n",
    "                 top_n,\n",
    "                 phraseVecModel=phraseVecModel,\n",
    "                 mapping=mapping,\n",
    "                 model=model):\n",
    "    \n",
    "    similarities = []\n",
    "    phrase_vec = phraseVecModel[phrase]\n",
    "    for idx in range(len(mapping)):\n",
    "        emoij_idx_similarity = model.nn.forward(torch.Tensor(phrase_vec.reshape(1, -1)), idx).detach().numpy()\n",
    "        similarities.append(emoij_idx_similarity)\n",
    "        \n",
    "    similarities = np.array(similarities)\n",
    "    n_most_similar_idxs = similarities.argsort(axis=0)[-top_n:][::-1].reshape(-1)\n",
    "    n_most_similar_emojis = [mapping[emoji_idx] for emoji_idx in n_most_similar_idxs]\n",
    "    \n",
    "    return phrase, n_most_similar_emojis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[\"Phrase\", \"Closest emojis\"],\n",
    "         [*phrase2emoji(\"minecraft\", 3)],\n",
    "         [*phrase2emoji(\"meme\", 3)],\n",
    "         [*phrase2emoji(\"a good project rating\", 3)],\n",
    "         [*phrase2emoji(\"natural language processing\", 3)],\n",
    "         [*phrase2emoji(\"the school project\", 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================  ========================\n",
      "Phrase                       Closest emojis\n",
      "===========================  ========================\n",
      "minecraft                    ['ğŸ™ğŸ¿', 'ğŸ', 'ğŸ’¸']\n",
      "meme                         ['ğŸ’¬', 'ğŸ‘…', 'ğŸ’¨']\n",
      "a good project rating        ['ğŸ’¯', 'ğŸ‘', 'ğŸ‡®ğŸ‡¸']\n",
      "natural language processing  ['ğŸ™ğŸ¿', 'ğŸ‘²ğŸ½', 'ğŸ‘†ğŸ¾']\n",
      "the school project           ['ğŸšŒ', 'ğŸ«', 'ğŸš']\n",
      "===========================  ========================\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"rst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to exist a strong relationship between simple adjectives and emojis. That's great, but is this the whole point of Emoji2Vec? <br>\n",
    "Of course not. In fact Emoji2Vec authors evaluated their method's performance on a downstream task of sentiment classification on a dataset  by  Kralj   Novak  et  al.  (2015),  which  consists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment. Using Emoji2Vec alongside word embeddings, as opposed to using just word embeddings, yielded an improvement in classification accuracy across all studied datasets. In the same task Emoji2Vec also outperformed an alternative method for emoji representation. <br>\n",
    "For more details check out the [paper](https://arxiv.org/pdf/1609.08359.pdf). You can also inspect the results of the sentiment classification task in [this notebook](https://github.com/uclmr/emoji2vec/blob/master/TwitterClassification.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
